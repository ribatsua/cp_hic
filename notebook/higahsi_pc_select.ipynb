{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#思路 按染色体对 整个数据集求和得伪bulk矩阵，借用dchic的pc值校正方法，定义单独的函数，输入为多个pc，输入为校正后的最终Pc值，所有的cell和bulk数据共用相同的投影矩阵\n",
    "import subprocess\n",
    "import sys\n",
    "sys.path.append('/home/higashi/Higashi/higashi')\n",
    "\n",
    "\n",
    "from Higashi_backend.utils import *\n",
    "from Higashi_analysis.Higashi_analysis import *\n",
    "import h5py\n",
    "from sklearn.preprocessing import MinMaxScaler, quantile_transform\n",
    "import os\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"10\"\n",
    "import pandas as pd\n",
    "import argparse\n",
    "\n",
    "def create_mask(k=30, chrom=\"chr1\", origin_sparse=None):\n",
    "\tfinal = np.array(np.sum(origin_sparse, axis=0).todense())\n",
    "\tsize = origin_sparse[0].shape[-1]\n",
    "\t#生成一个稀疏矩阵尺寸的全为0的矩阵\n",
    "\ta = np.zeros((size, size))\n",
    "\t#默认调用时传递k = 100000\n",
    "\t#如果该染色体被划分为小于10万个bin 则相当于还是全0矩阵\n",
    "\t#\n",
    "\tif k > 0:\n",
    "\t\tfor i in range(min(k, len(a))):\n",
    "\t\t\tfor j in range(len(a) - i):\n",
    "\t\t\t\ta[j, j + i] = 1\n",
    "\t\t\t\ta[j + i, j] = 1\n",
    "\t\ta = np.ones_like((a)) - a\n",
    "\n",
    "\t# print(a)\n",
    "\t# 对bulk矩阵每一行求和，如果对应行的和等于0 则为true 否则为false\n",
    "\tgap = np.sum(final, axis=-1, keepdims=False) == 0\n",
    "\n",
    "\t# 如果cytoband文件存在\n",
    "\tif cytoband_path is not None:\n",
    "\t\t#读取该文件为表格\n",
    "\t\tgap_tab = pd.read_table(cytoband_path, sep=\"\\t\", header=None)\n",
    "\t\t#定义列名\n",
    "\t\tgap_tab.columns = ['chrom', 'start', 'end', 'name', 'type']\n",
    "\t\t#获取name列\n",
    "\t\tname = np.array(gap_tab['name'])\n",
    "\t\t# print (name)\n",
    "\t\t#qC1.1\n",
    "\t\t#获取name 第一个字符\n",
    "\t\tpqarm = np.array([str(s)[0] for s in name])\n",
    "\t\t#作为gap_tab的新列\n",
    "\t\tgap_tab['pq_arm'] = pqarm\n",
    "\t\t#获取 区域长度作为新列\n",
    "\t\tgap_tab['length'] = gap_tab['end'] - gap_tab['start']\n",
    "\t\t#按照 'chrom' 和 'pq_arm' 分组并对数值列进行求和\n",
    "\t\tsummarize = gap_tab.groupby(['chrom', 'pq_arm']).sum().reset_index()\n",
    "\t\t# print (summarize)\n",
    "\t\t#如果求和结果中['pq_arm'] == 'p'的和大于0\n",
    "\t\t#则划分点为该染色体中 对符合条件（summarize['pq_arm'] == 'p')）的 'length' 列数据每个元素除以 分辨率 并向上取整后的第一个元素\n",
    "\t\tif np.sum(summarize['pq_arm'] == 'p') > 0:\n",
    "\t\t\tsplit_point = \\\n",
    "\t\t\tnp.ceil(np.array(summarize[(summarize['chrom'] == chrom) & (summarize['pq_arm'] == 'p')]['length']) / res)[0]\n",
    "\t\t#否则 置划分点为-1\n",
    "\t\telse:\n",
    "\t\t\tsplit_point = -1\n",
    "\t\t\n",
    "\t\tgap_list = gap_tab[(gap_tab[\"chrom\"] == chrom) & (gap_tab[\"type\"] == \"acen\")]\n",
    "\t\tstart = np.floor((np.array(gap_list['start'])) / res).astype('int')\n",
    "\t\tend = np.ceil((np.array(gap_list['end'])) / res).astype('int')\n",
    "\t\t\n",
    "\t\tfor s, e in zip(start, end):\n",
    "\t\t\ta[s:e, :] = 1\n",
    "\t\t\ta[:, s:e] = 1\n",
    "\t#如果cytoband文件不存在\n",
    "\t#直接置split_point = -1\n",
    "\telse:\n",
    "\t\tsplit_point = -1\n",
    "\t#将矩阵a 中gap为true的对应位置置1，如：gap第一个位置为true。则a的第一行 第一列置为1\n",
    "\ta[gap, :] = 1\n",
    "\ta[:, gap] = 1\n",
    "\t#返回结果a 和转换为整数的划分点\n",
    "\treturn a, int(split_point)\n",
    "\n",
    "\n",
    "def test_compartment(matrix, return_PCA=False, model=None, expected = None):\n",
    "\tcontact = matrix\n",
    "\t# np.fill_diagonal(contact, np.max(contact))\n",
    "\t# contact = KRnormalize(matrix)\n",
    "\t# contact[np.isnan(contact)] = 0.0\n",
    "\tcontact = sqrt_norm(matrix)\n",
    "\tcontact = oe(contact, expected)\n",
    "\tnp.fill_diagonal(contact, 1)\n",
    "\twith warnings.catch_warnings():\n",
    "\t\twarnings.filterwarnings(\n",
    "\t\t\t\"ignore\", category=PearsonRConstantInputWarning\n",
    "\t\t)\n",
    "\t\tcontact = pearson(contact)\n",
    "\tnp.fill_diagonal(contact, 1)\n",
    "\tcontact[np.isnan(contact)] = 0.0\n",
    "\tif model is not None:\n",
    "\t\ty = model.transform(contact)\n",
    "\telse:\n",
    "\t\t# pca = PCA(n_components=3)       #保留的pca的列数\n",
    "\t\tpca = PCA(n_components=2)\n",
    "\t\ty = pca.fit_transform(contact)\n",
    "\tif return_PCA:\n",
    "\t\treturn y, pca\n",
    "\telse:\n",
    "\t\treturn y\n",
    "\n",
    "def get_config(config_path = \"./config.jSON\"):\n",
    "\tc = open(config_path,\"r\")\n",
    "\treturn json.load(c)\n",
    "\n",
    "# res = 250000\n",
    "# temp_dir = '/home/dataset/snm3c/250k/raw'\n",
    "# output = 'scCompartment_snsm3c250k'\n",
    "# cytoband_path = '/home/annotation/hg19/cytoBand.txt'\n",
    "\n",
    "# chrom_list = ['chr1']\n",
    "# chrom_list = np.array(chrom_list)\n",
    "\n",
    "def download_file_with_curl(url, destination):\n",
    "    subprocess.run(['curl', '-O', url, '-L'], check=True, cwd=destination)\n",
    "\n",
    "\n",
    "def download_and_extract(genome, folder, resolution):\n",
    "    if folder is None or folder.strip() == \"\":\n",
    "        folder = f\"{genome}_{int(resolution)}_goldenpathData\"\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "            current_path = os.getcwd()\n",
    "            os.chdir(current_path)\n",
    "        folder = os.path.normpath(folder)\n",
    "    else:\n",
    "        folder = os.path.normpath(folder)\n",
    "\n",
    "    genome_fa_file = f\"{folder}/{genome}.fa\"\n",
    "    if not os.path.exists(f\"{folder}/cytoBand.txt.gz\"):\n",
    "        print(\"download cytoBand.txt.gz\")\n",
    "        download_file_with_curl(f\"http://hgdownload.cse.ucsc.edu/goldenPath/{genome}/database/cytoBand.txt.gz\", folder)\n",
    "        \n",
    "\n",
    "    if not os.path.exists(f\"{folder}/{genome}.chrom.sizes\"):\n",
    "        print(f\"download {genome}.chrom.sizes\")\n",
    "        download_file_with_curl(f\"http://hgdownload.cse.ucsc.edu/goldenPath/{genome}/bigZips/{genome}.chrom.sizes\", folder)\n",
    "    if not os.path.exists(f\"{folder}/{genome}.refGene.gtf.gz\"):\n",
    "        print(f\"download {genome}.refGene.gtf.gz\")\n",
    "        download_file_with_curl(f\"http://hgdownload.cse.ucsc.edu/goldenPath/{genome}/bigZips/genes/{genome}.refGene.gtf.gz\", folder)\n",
    "        \n",
    "\n",
    "    if not os.path.exists(f\"{folder}/{genome}.fa.gz\"):\n",
    "        genome_fa_url = f\"http://hgdownload.cse.ucsc.edu/goldenPath/{genome}/bigZips/{genome}.fa.gz\"\n",
    "        print(f\"download {genome}.fa.gz\")\n",
    "        download_file_with_curl(genome_fa_url, folder)\n",
    "    if not os.path.exists(f\"{folder}/{genome}.fa\"):  \n",
    "        print(f\"Unzipping {genome}.fa\")\n",
    "        #解压文件输出定向到{folder}文件夹下 {genome}.fa文件\n",
    "        subprocess.run(f\"gunzip -c {folder}/{genome}.fa.gz > {folder}/{genome}.fa\", shell=True, check=True)\n",
    "\n",
    "\n",
    "    tss_bed_file = f\"{folder}/{genome}.tss.bed\"\n",
    "    if not os.path.exists(tss_bed_file):\n",
    "        cmd = f\"gunzip -c {folder}/{genome}.refGene.gtf.gz | awk -v OFS='\\\\t' '{{if($3==\\\"transcript\\\"){{if($7==\\\"+\\\"){{print $1,$4,$4+1}}else{{print $1,$5-1,$5}}}}}}' | grep -v 'alt' | grep -v 'random' | sort |uniq |sort -k 1,1 -k2,2n > {folder}/{genome}.tss.bed\"\n",
    "    \n",
    "        print(f\"Running {cmd}\")\n",
    "        subprocess.run(cmd, shell=True, check=True)\n",
    "\n",
    "    binned_bed_file = f\"{folder}/{genome}.binned.bed\"\n",
    "    if not os.path.exists(binned_bed_file):\n",
    "        cmd = f\"bedtools makewindows -g {folder}/{genome}.chrom.sizes -w {int(resolution)} > {folder}/{genome}.binned.bed\"\n",
    "        print(f\"Running {cmd}\")\n",
    "        subprocess.run(cmd, shell=True, check=True)\n",
    "\n",
    "    gcpt_bedgraph_file = f\"{folder}/{genome}.GCpt.bedGraph\"\n",
    "    if not os.path.exists(gcpt_bedgraph_file):\n",
    "        cmd = f\"bedtools nuc -fi {folder}/{genome}.fa -bed  {folder}/{genome}.binned.bed | grep -v '#' | awk -v OFS='\\\\t' '{{print $1,$2,$3,$5}}' | grep -v 'alt' | grep -v 'random' | sort -k 1,1 -k2,2n > {folder}/{genome}.GCpt.bedGraph\"\n",
    "        print(f\"Running {cmd}\")\n",
    "        subprocess.run(cmd, shell=True, check=True)\n",
    "\n",
    "    gcpt_tss_bedgraph_file = f\"{folder}/{genome}.GCpt.tss.bedGraph\"\n",
    "    if not os.path.exists(gcpt_tss_bedgraph_file):\n",
    "        cmd = f\"bedtools map -a {folder}/{genome}.GCpt.bedGraph -b {folder}/{genome}.tss.bed -c 1 -o count -null 0 > {folder}/{genome}.GCpt.tss.bedGraph\"\n",
    "        print(f\"Running {cmd}\")\n",
    "        subprocess.run(cmd, shell=True, check=True)\n",
    "    print(\"GCpt.tss.bedGraph already exists\")\n",
    "    goldenpath = f\"{folder}/{genome}.GCpt.tss.bedGraph\"\n",
    "    return goldenpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download mm9.refGene.gtf.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 12.3M  100 12.3M    0     0  2440k      0  0:00:05  0:00:05 --:--:-- 3110k\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running gunzip -c mm9_250000_goldenpathData/mm9.refGene.gtf.gz | awk -v OFS='\\t' '{if($3==\"transcript\"){if($7==\"+\"){print $1,$4,$4+1}else{print $1,$5-1,$5}}}' | grep -v 'alt' | grep -v 'random' | sort |uniq |sort -k 1,1 -k2,2n > mm9_250000_goldenpathData/mm9.tss.bed\n",
      "Running bedtools makewindows -g mm9_250000_goldenpathData/mm9.chrom.sizes -w 250000 > mm9_250000_goldenpathData/mm9.binned.bed\n",
      "Running bedtools nuc -fi mm9_250000_goldenpathData/mm9.fa -bed  mm9_250000_goldenpathData/mm9.binned.bed | grep -v '#' | awk -v OFS='\\t' '{print $1,$2,$3,$5}' | grep -v 'alt' | grep -v 'random' | sort -k 1,1 -k2,2n > mm9_250000_goldenpathData/mm9.GCpt.bedGraph\n",
      "Running bedtools map -a mm9_250000_goldenpathData/mm9.GCpt.bedGraph -b mm9_250000_goldenpathData/mm9.tss.bed -c 1 -o count -null 0 > mm9_250000_goldenpathData/mm9.GCpt.tss.bedGraph\n",
      "GCpt.tss.bedGraph already exists\n"
     ]
    }
   ],
   "source": [
    "genome_name = \"mm9\"\n",
    "resolution_value = 250000\n",
    "pc_k = 2\n",
    "data_folder = None\n",
    "#下载参考基因组文件并生成 GCpt.tss.bedGraph文件（用于后续挑选pc）\n",
    "goldenpath = download_and_extract(genome_name, data_folder, resolution_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pc_flip_and_selection(df,use_rows):\n",
    "\n",
    "    gcc = pd.read_table(goldenpath, header=None, names=[\"chr\", \"start\", \"end\", \"gcc\", \"tss\"])\n",
    "    # 为GCC数据添加行名\n",
    "    gcc.index = gcc[\"chr\"].astype(str) + \"_\" + gcc[\"start\"].astype(str)\n",
    "    lhs = pd.DataFrame()\n",
    "    lhs[['pc1','pc2']] = df\n",
    "    lhs.index= ['chr1'+'_'+str(start*250000) for start in use_rows]\n",
    "    lhs[[\"gcc\", \"tss\"]] = gcc.loc[lhs.index, [\"gcc\", \"tss\"]]\n",
    "    lhs['len'] = range(1, lhs.shape[0] + 1)\n",
    "    # print(lhs)\n",
    "    pc_sign = []\n",
    "    pc_k = 2\n",
    "    for k in range(pc_k):\n",
    "            # 计算pc值和gcc的相关性 决定pc值是否乘以-1  \n",
    "        a = np.sign(pearsonr(lhs.iloc[:, k], lhs[\"gcc\"])[0])\n",
    "        lhs.iloc[:, k] = a*lhs.iloc[:, k]\n",
    "        pc_sign.append(a)\n",
    "    pc_flip= lhs.iloc[:,0:pc_k]\n",
    "\n",
    "    gcc_values=[pd.concat([pc_flip, lhs[\"gcc\"]],axis=1).corr().iloc[:-1, -1].round(4).transpose()]\n",
    "    tss_values=[pd.concat([pc_flip, lhs[\"tss\"]],axis=1).corr().iloc[:-1, -1].round(4).transpose()]\n",
    "    len_values=[pd.concat([pc_flip, lhs[\"len\"]],axis=1).corr().iloc[:-1, -1].round(4).transpose()]\n",
    "    score = [pc1+pc2 for pc1,pc2 in zip(gcc_values,tss_values)]\n",
    "    max_pc_index = score.index(max(score))\n",
    "    pc_flip = np.array(pc_flip.iloc[:,max_pc_index])\n",
    "    # pc_flip = pc_flip.iloc[:,max_pc_index]\n",
    "    pc_sign = pc_sign[max_pc_index]\n",
    "\n",
    "    return pc_flip,max_pc_index,pc_sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one_chrom(chrom):\n",
    "\n",
    "\tpc_select =None\n",
    "\tpc_flap = None\n",
    "\n",
    "\t# Get the raw sparse mtx list\n",
    "\tcell_type_info=pickle.load(open(os.path.join(temp_dir, \"label_info.pickle\"), \"rb\"))\n",
    "\tcell_type=cell_type_info['cell_type']\n",
    "\torigin_sparse = np.load(os.path.join(temp_dir, \"%s_sparse_adj.npy\" % chrom), allow_pickle=True)\n",
    "\tsize = origin_sparse[0].shape[0]\n",
    "\tprint(size)\n",
    "\t# find centromere & gaps...\n",
    "\tmask, split_point = create_mask((int(1e5)), chrom, origin_sparse)\n",
    "\n",
    "\tbulk1 = np.array(np.sum(origin_sparse, axis=0).todense())\n",
    "\tprint(bulk1.shape)\n",
    "\tmask = (np.ones_like(bulk1) - mask)\n",
    "\t# bulk1 *= mask\n",
    "\t# bulk1 *= mask\n",
    "\t# numpy.core._exceptions._UFuncOutputCastingError: Cannot cast ufunc 'multiply' output from dtype('float64') to dtype('int32') with casting rule 'same_kind'\n",
    "\tbulk1 *= mask.astype(np.int32)\n",
    "\tbulk2 =None\n",
    "\n",
    "\n",
    "\tuse_rows_all = []\n",
    "\n",
    "\tif split_point >= 20 * 1000000 / res:\n",
    "\t\tslice_start_list, slice_end_list = [0, split_point], [split_point, len(bulk1)]\n",
    "\telse:\n",
    "\t\tslice_start_list, slice_end_list = [0], [len(bulk1)]\n",
    "\n",
    "\tbulk_compartment_all = []\n",
    "\treal_bulk_compartment_all = []\n",
    "\ttemp_compartment_list_zscore = []\n",
    "\ttemp_compartment_list_quantile = []\n",
    "\n",
    "\tbulk_model_list = []\n",
    "\tbulk_reverse_list = []\n",
    "\tbulk_slice_list = []\n",
    "\tuse_rows_list = []\n",
    "\t\n",
    "\t# temp_compartment_list_zscore = []\n",
    "\t# temp_compartment_list_quantile = []\n",
    "\n",
    "\tfor slice_start, slice_end in zip(slice_start_list, slice_end_list):\n",
    "\t\t\n",
    "\t\tbulk1_slice = bulk1[slice_start:slice_end, :]\n",
    "\t\tbulk1_slice = bulk1_slice[:, slice_start:slice_end]\n",
    "\t\tuse_rows = np.where(np.sum(bulk1_slice > 0, axis=-1) > 0.01 * len(bulk1_slice))[0]\n",
    "\t\tif len(use_rows) <= 1:\n",
    "\t\t\tprint(\"no reliable bins in slice:\", slice_start, slice_end)\n",
    "\t\t\tcontinue\n",
    "\t\tuse_rows_all.append(np.arange(slice_start, slice_end)[use_rows])\n",
    "\t\tuse_rows_list.append(use_rows)\n",
    "\t\tbulk1_slice = bulk1_slice[use_rows, :]\n",
    "\t\tbulk1_slice = bulk1_slice[:, use_rows]\n",
    "\t\t# print('use_rows_list',len(use_rows_list))\n",
    "\t\t\n",
    "\t\tbulk_slice_list.append(bulk1_slice)\n",
    "\t\tbulk_expect = []\n",
    "\t\tfor k in range(len(bulk1_slice)):\n",
    "\t\t\tdiag = np.diag(bulk1_slice, k)\n",
    "\t\t\tbulk_expect.append(np.mean(diag))\n",
    "\t\t\n",
    "\t\t\n",
    "\t\tbulk_compartment, model = test_compartment(bulk1_slice, return_PCA=True)\n",
    "\t\t\t\n",
    "\t\t# reverse_flag = False\n",
    "\t\tbulk_compartment_all.append(bulk_compartment)\n",
    "\t\tbulk_model_list.append(model)\n",
    "\tbulk_compartment = np.concatenate(bulk_compartment_all, axis=0)\n",
    "\tuse_rows = np.concatenate(use_rows_all, axis=0)\n",
    "\t#bulk_compartment内容为Pc1 pc2\n",
    "\treal_bulk_compartment,max_pc_index,pc_sign= pc_flip_and_selection(bulk_compartment,use_rows)\n",
    "\tprint(bulk_compartment.shape)\n",
    "\tprint(len(use_rows_list))\n",
    "\t#开始计算每个cell的pc\n",
    "\ttemp_compartment_list_all = [[] for i in range(len(use_rows_list))]\n",
    "\tcell_list = trange(len(origin_sparse))\n",
    "\t# cell_list = trange(3)\n",
    "\t# print(cell_list)\n",
    "\ttemp = np.zeros((size, size))\n",
    "\tfor i in cell_list:\n",
    "\t\ttemp *= 0.0\n",
    "\t\tproba = np.array(origin_sparse[i].todense())\n",
    "\t\ttemp+= proba\n",
    "\t\ttemp = temp + temp.T\n",
    "\t\ttemp *= mask\n",
    "\t\tfor j in range(len(use_rows_list)):\n",
    "\t\t\tslice_start, slice_end = slice_start_list[j], slice_end_list[j]\n",
    "\t\t\ttemp_slice = temp[slice_start:slice_end, :]\n",
    "\t\t\ttemp_slice = temp_slice[:, slice_start:slice_end]\n",
    "\t\t\ttemp_select = temp_slice[use_rows_list[j], :]\n",
    "\t\t\ttemp_select = temp_select[:, use_rows_list[j]]\n",
    "\t\t\t# temp_select = rankmatch(temp_select, bulk_slice_list[j])\n",
    "\t\t\ttemp_compartment = test_compartment(temp_select, False, bulk_model_list[j], None)\n",
    "\t\t\ttemp_compartment = temp_compartment[:,max_pc_index]*pc_sign\n",
    "\t\t\t# print(type(temp_compartment))\n",
    "\t\t\t# print(len(use_rows_list[j]))\n",
    "\t\t\ttemp_compartment_list_all[j].append(temp_compartment.reshape((-1)))\n",
    "\t\t\t# temp_compartment_list_all[j].append(temp_compartment)\n",
    "\t\t\t\t\n",
    "\tfor j in range(len(use_rows_list)):\n",
    "\t\ttemp_compartment_list_all[j] = np.stack(temp_compartment_list_all[j], axis=0)\n",
    "\t\ttemp_compartment_list_quantile.append(quantile_transform(temp_compartment_list_all[j], output_distribution='uniform',\n",
    "\t\t                                           n_quantiles=int(temp_compartment_list_all[j].shape[-1] * 1.0), axis=1))\n",
    "\t\t\n",
    "\t\ttemp_compartment_list_zscore.append(zscore(temp_compartment_list_all[j], axis=1))\n",
    "\ttemp_compartment_list = np.concatenate(temp_compartment_list_all, axis=-1)\n",
    "\n",
    "\ttemp_compartment_list_zscore = np.concatenate(temp_compartment_list_zscore, axis=-1)\n",
    "\ttemp_compartment_list_quantile = np.concatenate(temp_compartment_list_quantile, axis=-1)\n",
    "\t#按细胞类型计算伪bulk的pc值 用于输入dchic\n",
    "\torigin_bulk_list_all=[[] for i in range(len(use_rows_list))]\n",
    "\t# sum_by_label=[]\n",
    "\ta={}\n",
    "\tb={}\n",
    "\tfor j in list(set(cell_type)):\n",
    "\t\t\n",
    "\t\tindices = [index for index, value in enumerate(cell_type) if value == j]\n",
    "\t\ta[j]=indices\n",
    "\t\tb[j] = np.zeros_like(np.array(origin_sparse[0]))\n",
    "\t\tfor i in a[j]:\n",
    "\t\t\t\tproba = np.array(origin_sparse[i])    \n",
    "\t\t\t\tb[j] +=proba\n",
    "\t\ttemp = np.array(b[j].item().todense())\n",
    "\t\t# print(temp.shape)\n",
    "\t\ttemp *= mask\n",
    "\t\tfor j in range(len(use_rows_list)):\n",
    "\t\t\tslice_start, slice_end = slice_start_list[j], slice_end_list[j]\n",
    "\t\t\ttemp_slice = temp[slice_start:slice_end, :]\n",
    "\t\t\ttemp_slice = temp_slice[:, slice_start:slice_end]\n",
    "\t\t\ttemp_select = temp_slice[use_rows_list[j], :]\n",
    "\t\t\ttemp_select = temp_select[:, use_rows_list[j]]\n",
    "\t\t\t# temp_select = rankmatch(temp_select, bulk_slice_list[j])\n",
    "\t\t\ttemp_compartment = test_compartment(temp_select, False, bulk_model_list[j], None)\n",
    "\t\t\ttemp_compartment = temp_compartment[:,max_pc_index]*pc_sign #pc值校正\n",
    "\t\t\t\n",
    "\t\t\t# temp_compartment_list_all[j].append(temp_compartment.reshape((-1)))\n",
    "\t\t\torigin_bulk_list_all[j].append(temp_compartment.reshape((-1)))\n",
    "        \n",
    "\tfor j in range(len(use_rows_list)):\n",
    "\t\torigin_bulk_list_all[j] = np.stack(origin_bulk_list_all[j], axis=0)\n",
    "\tpresudo_bulk_list = np.concatenate(origin_bulk_list_all, axis=-1)\n",
    "\tprint (chrom, \"finished\")\n",
    "\t\n",
    "\treturn presudo_bulk_list, temp_compartment_list,temp_compartment_list_zscore,temp_compartment_list_quantile,use_rows,size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/dataset/snm3c/250k/raw'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_call_compartment(output):\n",
    "\n",
    "\t\n",
    "\tif \".hdf5\" not in output:\n",
    "\t\toutput += \".hdf5\"\n",
    "\twith h5py.File(os.path.join(temp_dir, output), \"w\") as output_f:\n",
    "\t\tresult = {}\n",
    "\t\tfor chrom in chrom_list:\n",
    "\t\t\tpresudo_bulk_list, temp_compartment_list,temp_compartment_list_zscore,temp_compartment_list_quantile,use_rows,size = process_one_chrom(chrom)\n",
    "\t\t\tresult[chrom] = [presudo_bulk_list, temp_compartment_list,temp_compartment_list_zscore,temp_compartment_list_quantile,use_rows,size]\n",
    "\t\t\t\n",
    "\t\tbin_chrom_list = []\n",
    "\t\tbin_start_list = []\n",
    "\t\tbin_end_list = []\n",
    "\t\tpresudo_bulk_all = []\n",
    "\t\tsc_cp_raw = []\n",
    "\t\tsc_cp_zscore = []\n",
    "\t\tsc_cp_quantile = []\n",
    "\t\t#####################################整个数据集的bulk值##########################################################\n",
    "\t\t# grp = output_f.create_group('compartment')\n",
    "\t\t# bin = grp.create_group('bin')\n",
    "\t\t#############################################################################################################\n",
    "\t\t\n",
    "\t\tfor chrom in chrom_list:\n",
    "\t\t\tpresudo_bulk_list, temp_compartment_list,temp_compartment_list_zscore,temp_compartment_list_quantile,use_rows,size= result[chrom]\n",
    "\t\t\t# print (use_rows)\n",
    "\t\t\t# print(chrom)\n",
    "\t\t\tlength = size\n",
    "\t\t\tbin_chrom_list += [chrom] * len(use_rows)\n",
    "\t\t\tbin_start_list.append((np.arange(length) * res).astype('int')[use_rows])\n",
    "\t\t\tbin_end_list.append(((np.arange(length) + 1) * res).astype('int')[use_rows])\n",
    "\t\t\tpresudo_bulk_all.append(presudo_bulk_list)\n",
    "\t\t\tsc_cp_raw.append(temp_compartment_list)\n",
    "\t\t\tsc_cp_zscore.append(temp_compartment_list_zscore)\n",
    "\t\t\tsc_cp_quantile.append(temp_compartment_list_quantile)\n",
    "\t\t\t\n",
    "\t\tpresudo_bulk_all = np.concatenate(presudo_bulk_all, axis=-1)\n",
    "\t\tprint('presudo_bulk_all.shape:',presudo_bulk_all.shape)\n",
    "\t\tsc_cp_raw = np.concatenate(sc_cp_raw, axis=-1)\n",
    "\t\tsc_cp_zscore = np.concatenate(sc_cp_zscore, axis=-1)\n",
    "\t\tsc_cp_quantile=np.concatenate(sc_cp_quantile, axis=-1)\n",
    "\t\t\n",
    "\t\tgrp = output_f.create_group('compartment_raw')\n",
    "\t\tbin = grp.create_group('bin')\n",
    "\t\tbin.create_dataset('chrom', data=[l.encode('utf8') for l in bin_chrom_list],\n",
    "\t\t                   dtype=h5py.special_dtype(vlen=str))\n",
    "\t\tbin.create_dataset('start', data=np.concatenate(bin_start_list))\n",
    "\t\tbin.create_dataset('end', data=np.concatenate(bin_end_list))\n",
    "\t\tfor cell in range(len(sc_cp_raw)):\n",
    "\t\t\tgrp.create_dataset(\"cell_%d\" % cell, data=sc_cp_raw[cell])\n",
    "\n",
    "\t\tgrp = output_f.create_group('compartment_zscore')\n",
    "\t\tbin = grp.create_group('bin')\n",
    "\t\tbin.create_dataset('chrom', data=[l.encode('utf8') for l in bin_chrom_list],\n",
    "\t\t                   dtype=h5py.special_dtype(vlen=str))\n",
    "\t\tbin.create_dataset('start', data=np.concatenate(bin_start_list))\n",
    "\t\tbin.create_dataset('end', data=np.concatenate(bin_end_list))\n",
    "\t\tfor cell in range(len(sc_cp_zscore)):\n",
    "\t\t\tgrp.create_dataset(\"cell_%d\" % cell, data=sc_cp_zscore[cell])\n",
    "\n",
    "\t\tgrp = output_f.create_group('compartment')\n",
    "\t\tbin = grp.create_group('bin')\n",
    "\t\tbin.create_dataset('chrom', data=[l.encode('utf8') for l in bin_chrom_list],\n",
    "\t\t                   dtype=h5py.special_dtype(vlen=str))\n",
    "\t\tbin.create_dataset('start', data=np.concatenate(bin_start_list))\n",
    "\t\tbin.create_dataset('end', data=np.concatenate(bin_end_list))\n",
    "\t\tfor cell in range(len(sc_cp_quantile)):\n",
    "\t\t\tgrp.create_dataset(\"cell_%d\" % cell, data=sc_cp_quantile[cell])\n",
    "\n",
    "\n",
    "\n",
    "\t\tgrp = output_f.create_group('compartment_presudo_bulk')\n",
    "\t\tbin = grp.create_group('bin')\n",
    "\t\tbin.create_dataset('chrom', data=[l.encode('utf8') for l in bin_chrom_list],\n",
    "\t\t                   dtype=h5py.special_dtype(vlen=str))\n",
    "\t\tbin.create_dataset('start', data=np.concatenate(bin_start_list))\n",
    "\t\tbin.create_dataset('end', data=np.concatenate(bin_end_list))\n",
    "\t\tcell_type_info=pickle.load(open(os.path.join(temp_dir, \"label_info.pickle\"), \"rb\"))\n",
    "\t\tcell_type=cell_type_info['cell_type']\n",
    "\t\tcell_info = list(set(cell_type))\n",
    "\t\tfor j in range(len(cell_info)):\n",
    "\t\t\tgrp.create_dataset(cell_info[j], data=presudo_bulk_all[j,:])\n",
    "\t\t\t\n",
    "\toutput_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "789\n",
      "(789, 789)\n",
      "(769, 2)\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1171/1171 [03:36<00:00,  5.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr1 finished\n",
      "727\n",
      "(727, 727)\n",
      "(710, 2)\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1171/1171 [03:14<00:00,  6.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr2 finished\n",
      "639\n",
      "(639, 639)\n",
      "(627, 2)\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1171/1171 [02:49<00:00,  6.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr3 finished\n",
      "623\n",
      "(623, 623)\n",
      "(608, 2)\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1171/1171 [02:46<00:00,  7.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr4 finished\n",
      "611\n",
      "(611, 611)\n",
      "(590, 2)\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1171/1171 [02:42<00:00,  7.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr5 finished\n",
      "599\n",
      "(599, 599)\n",
      "(587, 2)\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1171/1171 [02:42<00:00,  7.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr6 finished\n",
      "611\n",
      "(611, 611)\n",
      "(556, 2)\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1171/1171 [02:36<00:00,  7.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr7 finished\n",
      "527\n",
      "(527, 527)\n",
      "(502, 2)\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1171/1171 [02:26<00:00,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr8 finished\n",
      "497\n",
      "(497, 497)\n",
      "(485, 2)\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1171/1171 [02:22<00:00,  8.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr9 finished\n",
      "520\n",
      "(520, 520)\n",
      "(508, 2)\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1171/1171 [02:26<00:00,  8.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr10 finished\n",
      "488\n",
      "(488, 488)\n",
      "(476, 2)\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1171/1171 [02:15<00:00,  8.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr11 finished\n",
      "486\n",
      "(486, 486)\n",
      "(470, 2)\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1171/1171 [02:15<00:00,  8.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr12 finished\n",
      "482\n",
      "(482, 482)\n",
      "(468, 2)\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1171/1171 [02:11<00:00,  8.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr13 finished\n",
      "501\n",
      "(501, 501)\n",
      "(483, 2)\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1171/1171 [02:19<00:00,  8.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr14 finished\n",
      "414\n",
      "(414, 414)\n",
      "(402, 2)\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1171/1171 [01:46<00:00, 10.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr15 finished\n",
      "394\n",
      "(394, 394)\n",
      "(382, 2)\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1171/1171 [01:40<00:00, 11.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr16 finished\n",
      "382\n",
      "(382, 382)\n",
      "(371, 2)\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1171/1171 [01:38<00:00, 11.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr17 finished\n",
      "364\n",
      "(364, 364)\n",
      "(352, 2)\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1171/1171 [01:07<00:00, 17.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr18 finished\n",
      "246\n",
      "(246, 246)\n",
      "(234, 2)\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1171/1171 [00:23<00:00, 49.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr19 finished\n",
      "667\n",
      "(667, 667)\n",
      "(636, 2)\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1171/1171 [02:54<00:00,  6.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chrX finished\n",
      "presudo_bulk_all.shape: (4, 10216)\n"
     ]
    }
   ],
   "source": [
    "temp_dir = '/home/dataset/cellcycle/sparse_input/250k/raw'\n",
    "cytoband_path = '/home/annotation/mm9/cytoBand.txt'\n",
    "res = 250000\n",
    "output = 'compartment250k'\n",
    "chrom_list = [\"chr1\",\"chr2\",\"chr3\",\"chr4\",\"chr5\",\n",
    "\t\t\t  \"chr6\",\"chr7\",\"chr8\",\"chr9\",\"chr10\",\n",
    "\t\t\t  \"chr11\",\"chr12\",\"chr13\",\"chr14\",\"chr15\",\n",
    "\t\t\t  \"chr16\",\"chr17\",\"chr18\",\"chr19\",\"chrX\"]\n",
    "start_call_compartment(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "998\n",
      "(998, 998)\n",
      "                     pc1       pc2       gcc  tss  len\n",
      "chr1_0          2.775440  2.079056  0.324984    8    1\n",
      "chr1_250000     2.018534  3.482645  0.287844    2    2\n",
      "chr1_500000     2.030299  1.674580  0.392268    5    3\n",
      "chr1_750000    -3.298628  2.530375  0.567700   17    4\n",
      "chr1_1000000   -5.836934  3.942275  0.612900   22    5\n",
      "...                  ...       ...       ...  ...  ...\n",
      "chr1_248000000 -5.273661  3.806374  0.387672   12  909\n",
      "chr1_248250000 -5.327095  3.586650  0.372096    8  910\n",
      "chr1_248500000 -4.522876  3.032983  0.378568   11  911\n",
      "chr1_248750000 -4.222943  3.003602  0.250820    7  912\n",
      "chr1_249000000  3.321932  0.622575  0.328424    7  913\n",
      "\n",
      "[913 rows x 5 columns]\n",
      "(913, 2)\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4238/4238 [17:38<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr1 finished\n"
     ]
    }
   ],
   "source": [
    "presudo_bulk_list,real_bulk_compartment,temp_compartment_list,temp_compartment_list_zscore,temp_compartment_list_quantile,use_rows = process_one_chrom('chr1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4238, 913)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_compartment_list_zscore.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 913)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "presudo_bulk_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 913)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_compartment_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "913"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(use_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_pc_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bulk_compartment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mbulk_compartment\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bulk_compartment' is not defined"
     ]
    }
   ],
   "source": [
    "        pca[[\"gcc\", \"tss\"]] = gcc.loc[pca.index, [\"gcc\", \"tss\"]]\n",
    "        pca['len'] = range(1, pca.shape[0] + 1)\n",
    "        # print(pca.iloc[:, 4])\n",
    "        # 生成bedGraph文件\n",
    "        for k in range(4, pca.shape[1] - 3):\n",
    "\n",
    "            # 计算pc值和gcc的相关性 决定pc值是否乘以-1\n",
    "            pca.iloc[:, k] = np.sign(pearsonr(pca.iloc[:, k], pca[\"gcc\"])[0])*pca.iloc[:, k]\n",
    "\n",
    "        # 存储染色体的pc值(正负校正后)\n",
    "        chrom_list[j] = pca.iloc[:, 4:(pca.shape[1] - 3)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chr1_0            2.079056\n",
       "chr1_250000       3.482645\n",
       "chr1_500000       1.674580\n",
       "chr1_750000       2.530375\n",
       "chr1_1000000      3.942275\n",
       "                    ...   \n",
       "chr1_227000000    3.806374\n",
       "chr1_227250000    3.586650\n",
       "chr1_227500000    3.032983\n",
       "chr1_227750000    3.003602\n",
       "chr1_228000000    0.622575\n",
       "Name: pc2, Length: 913, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lhs.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pc1</th>\n",
       "      <th>pc2</th>\n",
       "      <th>gcc</th>\n",
       "      <th>tss</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>chr1_0</th>\n",
       "      <td>2.775440</td>\n",
       "      <td>2.079056</td>\n",
       "      <td>0.324984</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr1_250000</th>\n",
       "      <td>2.018534</td>\n",
       "      <td>3.482645</td>\n",
       "      <td>0.287844</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr1_500000</th>\n",
       "      <td>2.030299</td>\n",
       "      <td>1.674580</td>\n",
       "      <td>0.392268</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr1_750000</th>\n",
       "      <td>-3.298628</td>\n",
       "      <td>2.530375</td>\n",
       "      <td>0.567700</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr1_1000000</th>\n",
       "      <td>-5.836934</td>\n",
       "      <td>3.942275</td>\n",
       "      <td>0.612900</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr1_227000000</th>\n",
       "      <td>-5.273661</td>\n",
       "      <td>3.806374</td>\n",
       "      <td>0.446612</td>\n",
       "      <td>3</td>\n",
       "      <td>909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr1_227250000</th>\n",
       "      <td>-5.327095</td>\n",
       "      <td>3.586650</td>\n",
       "      <td>0.365612</td>\n",
       "      <td>0</td>\n",
       "      <td>910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr1_227500000</th>\n",
       "      <td>-4.522876</td>\n",
       "      <td>3.032983</td>\n",
       "      <td>0.435116</td>\n",
       "      <td>1</td>\n",
       "      <td>911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr1_227750000</th>\n",
       "      <td>-4.222943</td>\n",
       "      <td>3.003602</td>\n",
       "      <td>0.445060</td>\n",
       "      <td>8</td>\n",
       "      <td>912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr1_228000000</th>\n",
       "      <td>3.321932</td>\n",
       "      <td>0.622575</td>\n",
       "      <td>0.501212</td>\n",
       "      <td>6</td>\n",
       "      <td>913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>913 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     pc1       pc2       gcc  tss  len\n",
       "chr1_0          2.775440  2.079056  0.324984    8    1\n",
       "chr1_250000     2.018534  3.482645  0.287844    2    2\n",
       "chr1_500000     2.030299  1.674580  0.392268    5    3\n",
       "chr1_750000    -3.298628  2.530375  0.567700   17    4\n",
       "chr1_1000000   -5.836934  3.942275  0.612900   22    5\n",
       "...                  ...       ...       ...  ...  ...\n",
       "chr1_227000000 -5.273661  3.806374  0.446612    3  909\n",
       "chr1_227250000 -5.327095  3.586650  0.365612    0  910\n",
       "chr1_227500000 -4.522876  3.032983  0.435116    1  911\n",
       "chr1_227750000 -4.222943  3.003602  0.445060    8  912\n",
       "chr1_228000000  3.321932  0.622575  0.501212    6  913\n",
       "\n",
       "[913 rows x 5 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lhs = pd.DataFrame()\n",
    "lhs[['pc1','pc2']] = bulk_compartment\n",
    "lhs.index= ['chr1'+'_'+str(start*250000) for start in range(bulk_compartment.shape[0])]\n",
    "lhs[[\"gcc\", \"tss\"]] = gcc.loc[lhs.index, [\"gcc\", \"tss\"]]\n",
    "lhs['len'] = range(1, lhs.shape[0] + 1)\n",
    "lhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_sign = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_k = 2\n",
    "for k in range(pc_k):\n",
    "         # 计算pc值和gcc的相关性 决定pc值是否乘以-1  \n",
    "     a = np.sign(pearsonr(lhs.iloc[:, k], lhs[\"gcc\"])[0])\n",
    "     lhs.iloc[:, k] = a*lhs.iloc[:, k]\n",
    "     pc_sign.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc_sign[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0\n"
     ]
    }
   ],
   "source": [
    "a = np.sign(pearsonr(lhs.iloc[:, 0], lhs[\"gcc\"])[0])\n",
    "# b=a*2\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pc1</th>\n",
       "      <th>pc2</th>\n",
       "      <th>gcc</th>\n",
       "      <th>tss</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>chr1_0</th>\n",
       "      <td>-2.775440</td>\n",
       "      <td>-2.079056</td>\n",
       "      <td>0.324984</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr1_250000</th>\n",
       "      <td>-2.018534</td>\n",
       "      <td>-3.482645</td>\n",
       "      <td>0.287844</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr1_500000</th>\n",
       "      <td>-2.030299</td>\n",
       "      <td>-1.674580</td>\n",
       "      <td>0.392268</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr1_750000</th>\n",
       "      <td>3.298628</td>\n",
       "      <td>-2.530375</td>\n",
       "      <td>0.567700</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr1_1000000</th>\n",
       "      <td>5.836934</td>\n",
       "      <td>-3.942275</td>\n",
       "      <td>0.612900</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr1_227000000</th>\n",
       "      <td>5.273661</td>\n",
       "      <td>-3.806374</td>\n",
       "      <td>0.446612</td>\n",
       "      <td>3</td>\n",
       "      <td>909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr1_227250000</th>\n",
       "      <td>5.327095</td>\n",
       "      <td>-3.586650</td>\n",
       "      <td>0.365612</td>\n",
       "      <td>0</td>\n",
       "      <td>910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr1_227500000</th>\n",
       "      <td>4.522876</td>\n",
       "      <td>-3.032983</td>\n",
       "      <td>0.435116</td>\n",
       "      <td>1</td>\n",
       "      <td>911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr1_227750000</th>\n",
       "      <td>4.222943</td>\n",
       "      <td>-3.003602</td>\n",
       "      <td>0.445060</td>\n",
       "      <td>8</td>\n",
       "      <td>912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr1_228000000</th>\n",
       "      <td>-3.321932</td>\n",
       "      <td>-0.622575</td>\n",
       "      <td>0.501212</td>\n",
       "      <td>6</td>\n",
       "      <td>913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>913 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     pc1       pc2       gcc  tss  len\n",
       "chr1_0         -2.775440 -2.079056  0.324984    8    1\n",
       "chr1_250000    -2.018534 -3.482645  0.287844    2    2\n",
       "chr1_500000    -2.030299 -1.674580  0.392268    5    3\n",
       "chr1_750000     3.298628 -2.530375  0.567700   17    4\n",
       "chr1_1000000    5.836934 -3.942275  0.612900   22    5\n",
       "...                  ...       ...       ...  ...  ...\n",
       "chr1_227000000  5.273661 -3.806374  0.446612    3  909\n",
       "chr1_227250000  5.327095 -3.586650  0.365612    0  910\n",
       "chr1_227500000  4.522876 -3.032983  0.435116    1  911\n",
       "chr1_227750000  4.222943 -3.003602  0.445060    8  912\n",
       "chr1_228000000 -3.321932 -0.622575  0.501212    6  913\n",
       "\n",
       "[913 rows x 5 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pc1</th>\n",
       "      <th>pc2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>chr1_0</th>\n",
       "      <td>-2.775440</td>\n",
       "      <td>-2.079056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr1_250000</th>\n",
       "      <td>-2.018534</td>\n",
       "      <td>-3.482645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr1_500000</th>\n",
       "      <td>-2.030299</td>\n",
       "      <td>-1.674580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr1_750000</th>\n",
       "      <td>3.298628</td>\n",
       "      <td>-2.530375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr1_1000000</th>\n",
       "      <td>5.836934</td>\n",
       "      <td>-3.942275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr1_227000000</th>\n",
       "      <td>5.273661</td>\n",
       "      <td>-3.806374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr1_227250000</th>\n",
       "      <td>5.327095</td>\n",
       "      <td>-3.586650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr1_227500000</th>\n",
       "      <td>4.522876</td>\n",
       "      <td>-3.032983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr1_227750000</th>\n",
       "      <td>4.222943</td>\n",
       "      <td>-3.003602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr1_228000000</th>\n",
       "      <td>-3.321932</td>\n",
       "      <td>-0.622575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>913 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     pc1       pc2\n",
       "chr1_0         -2.775440 -2.079056\n",
       "chr1_250000    -2.018534 -3.482645\n",
       "chr1_500000    -2.030299 -1.674580\n",
       "chr1_750000     3.298628 -2.530375\n",
       "chr1_1000000    5.836934 -3.942275\n",
       "...                  ...       ...\n",
       "chr1_227000000  5.273661 -3.806374\n",
       "chr1_227250000  5.327095 -3.586650\n",
       "chr1_227500000  4.522876 -3.032983\n",
       "chr1_227750000  4.222943 -3.003602\n",
       "chr1_228000000 -3.321932 -0.622575\n",
       "\n",
       "[913 rows x 2 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc_flip= lhs.iloc[:,0:pc_k]\n",
    "pc_flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_flip= lhs.iloc[:,0:pc_k]\n",
    "\n",
    "gcc_values=[pd.concat([pc_flip, lhs[\"gcc\"]],axis=1).corr().iloc[:-1, -1].round(4).transpose()]\n",
    "        # print(gcc_values)\n",
    "tss_values=[pd.concat([pc_flip, lhs[\"tss\"]],axis=1).corr().iloc[:-1, -1].round(4).transpose()]\n",
    "len_values=[pd.concat([pc_flip, lhs[\"len\"]],axis=1).corr().iloc[:-1, -1].round(4).transpose()]\n",
    "\n",
    "score = [pc1+pc2 for pc1,pc2 in zip(gcc_values,tss_values)]\n",
    "\n",
    "max_pc_index = score.index(max(score))\n",
    "\n",
    "pc_flip = np.array(pc_flip.iloc[:,max_pc_index])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pc1    0.3532\n",
      "pc2    0.0510\n",
      "Name: gcc, dtype: float64]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[pc1    0.3437\n",
       " pc2   -0.0353\n",
       " Name: tss, dtype: float64]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(gcc_values)\n",
    "tss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_chrom['gcc.cor']+vals_chrom['tss.cor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[pc1    0.6969\n",
       " pc2    0.0157\n",
       " dtype: float64]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "score = [pc1+pc2 for pc1,pc2 in zip(gcc_values,tss_values)]\n",
    "score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_pc_index = score.index(max(score))\n",
    "max_pc_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "913"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.array(pc_flip.iloc[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function list.index(value, start=0, stop=9223372036854775807, /)>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcc = pd.read_table(goldenpath, header=None, names=[\"chr\", \"start\", \"end\", \"gcc\", \"tss\"])\n",
    "# 为GCC数据添加行名\n",
    "gcc.index = gcc[\"chr\"].astype(str) + \"_\" + gcc[\"start\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chr</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>gcc</th>\n",
       "      <th>tss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>chr1_0</th>\n",
       "      <td>chr1</td>\n",
       "      <td>0</td>\n",
       "      <td>250000</td>\n",
       "      <td>0.324984</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr1_250000</th>\n",
       "      <td>chr1</td>\n",
       "      <td>250000</td>\n",
       "      <td>500000</td>\n",
       "      <td>0.287844</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr1_500000</th>\n",
       "      <td>chr1</td>\n",
       "      <td>500000</td>\n",
       "      <td>750000</td>\n",
       "      <td>0.392268</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr1_750000</th>\n",
       "      <td>chr1</td>\n",
       "      <td>750000</td>\n",
       "      <td>1000000</td>\n",
       "      <td>0.567700</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr1_1000000</th>\n",
       "      <td>chr1</td>\n",
       "      <td>1000000</td>\n",
       "      <td>1250000</td>\n",
       "      <td>0.612900</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chrY_58250000</th>\n",
       "      <td>chrY</td>\n",
       "      <td>58250000</td>\n",
       "      <td>58500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chrY_58500000</th>\n",
       "      <td>chrY</td>\n",
       "      <td>58500000</td>\n",
       "      <td>58750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chrY_58750000</th>\n",
       "      <td>chrY</td>\n",
       "      <td>58750000</td>\n",
       "      <td>59000000</td>\n",
       "      <td>0.210224</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chrY_59000000</th>\n",
       "      <td>chrY</td>\n",
       "      <td>59000000</td>\n",
       "      <td>59250000</td>\n",
       "      <td>0.382952</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chrY_59250000</th>\n",
       "      <td>chrY</td>\n",
       "      <td>59250000</td>\n",
       "      <td>59373566</td>\n",
       "      <td>0.416935</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12583 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                chr     start       end       gcc  tss\n",
       "chr1_0         chr1         0    250000  0.324984    8\n",
       "chr1_250000    chr1    250000    500000  0.287844    2\n",
       "chr1_500000    chr1    500000    750000  0.392268    5\n",
       "chr1_750000    chr1    750000   1000000  0.567700   17\n",
       "chr1_1000000   chr1   1000000   1250000  0.612900   22\n",
       "...             ...       ...       ...       ...  ...\n",
       "chrY_58250000  chrY  58250000  58500000  0.000000    0\n",
       "chrY_58500000  chrY  58500000  58750000  0.000000    0\n",
       "chrY_58750000  chrY  58750000  59000000  0.210224    0\n",
       "chrY_59000000  chrY  59000000  59250000  0.382952    2\n",
       "chrY_59250000  chrY  59250000  59373566  0.416935    3\n",
       "\n",
       "[12583 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "998\n",
      "(998, 998)\n",
      "(486, 2)\n",
      "(427, 2)\n",
      "(913, 2)\n"
     ]
    }
   ],
   "source": [
    "bulk_compartment = process_one_chrom('chr1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "'''\n",
    "@File    :   PC_selection_and _flipping.py\n",
    "@Time    :   2024/01/05 16:05:47\n",
    "@Author  :   cp \n",
    "@Version :   1.0\n",
    "@Desc    :   pc挑选和翻转\n",
    "'''\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import h5py \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from itertools import chain\n",
    "import re\n",
    "\n",
    "\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.packages import importr\n",
    "\n",
    "\n",
    "# 激活自动转换为pandas数据框\n",
    "pandas2ri.activate()\n",
    "\n",
    "# 安装并导入必要的R包\n",
    "base = importr('base')\n",
    "stats = importr('stats')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def download_file_with_curl(url, destination):\n",
    "    subprocess.run(['curl', '-O', url, '-L'], check=True, cwd=destination)\n",
    "\n",
    "\n",
    "def download_and_extract(genome, folder, resolution):\n",
    "    if folder is None or folder.strip() == \"\":\n",
    "        folder = f\"{genome}_{int(resolution)}_goldenpathData\"\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "            current_path = os.getcwd()\n",
    "            os.chdir(current_path)\n",
    "        folder = os.path.normpath(folder)\n",
    "    else:\n",
    "        folder = os.path.normpath(folder)\n",
    "\n",
    "    genome_fa_file = f\"{folder}/{genome}.fa\"\n",
    "    if not os.path.exists(f\"{folder}/cytoBand.txt.gz\"):\n",
    "        print(\"download cytoBand.txt.gz\")\n",
    "        download_file_with_curl(f\"http://hgdownload.cse.ucsc.edu/goldenPath/{genome}/database/cytoBand.txt.gz\", folder)\n",
    "        \n",
    "\n",
    "    if not os.path.exists(f\"{folder}/{genome}.chrom.sizes\"):\n",
    "        print(f\"download {genome}.chrom.sizes\")\n",
    "        download_file_with_curl(f\"http://hgdownload.cse.ucsc.edu/goldenPath/{genome}/bigZips/{genome}.chrom.sizes\", folder)\n",
    "    if not os.path.exists(f\"{folder}/{genome}.refGene.gtf.gz\"):\n",
    "        print(f\"download {genome}.refGene.gtf.gz\")\n",
    "        download_file_with_curl(f\"http://hgdownload.cse.ucsc.edu/goldenPath/{genome}/bigZips/genes/{genome}.refGene.gtf.gz\", folder)\n",
    "        \n",
    "\n",
    "    if not os.path.exists(f\"{folder}/{genome}.fa.gz\"):\n",
    "        genome_fa_url = f\"http://hgdownload.cse.ucsc.edu/goldenPath/{genome}/bigZips/{genome}.fa.gz\"\n",
    "        print(f\"download {genome}.fa.gz\")\n",
    "        download_file_with_curl(genome_fa_url, folder)\n",
    "    if not os.path.exists(f\"{folder}/{genome}.fa\"):  \n",
    "        print(f\"Unzipping {genome}.fa\")\n",
    "        #解压文件输出定向到{folder}文件夹下 {genome}.fa文件\n",
    "        subprocess.run(f\"gunzip -c {folder}/{genome}.fa.gz > {folder}/{genome}.fa\", shell=True, check=True)\n",
    "\n",
    "\n",
    "    tss_bed_file = f\"{folder}/{genome}.tss.bed\"\n",
    "    if not os.path.exists(tss_bed_file):\n",
    "        cmd = f\"gunzip -c {folder}/{genome}.refGene.gtf.gz | awk -v OFS='\\\\t' '{{if($3==\\\"transcript\\\"){{if($7==\\\"+\\\"){{print $1,$4,$4+1}}else{{print $1,$5-1,$5}}}}}}' | grep -v 'alt' | grep -v 'random' | sort |uniq |sort -k 1,1 -k2,2n > {folder}/{genome}.tss.bed\"\n",
    "    \n",
    "        print(f\"Running {cmd}\")\n",
    "        subprocess.run(cmd, shell=True, check=True)\n",
    "\n",
    "    binned_bed_file = f\"{folder}/{genome}.binned.bed\"\n",
    "    if not os.path.exists(binned_bed_file):\n",
    "        cmd = f\"bedtools makewindows -g {folder}/{genome}.chrom.sizes -w {int(resolution)} > {folder}/{genome}.binned.bed\"\n",
    "        print(f\"Running {cmd}\")\n",
    "        subprocess.run(cmd, shell=True, check=True)\n",
    "\n",
    "    gcpt_bedgraph_file = f\"{folder}/{genome}.GCpt.bedGraph\"\n",
    "    if not os.path.exists(gcpt_bedgraph_file):\n",
    "        cmd = f\"bedtools nuc -fi {folder}/{genome}.fa -bed  {folder}/{genome}.binned.bed | grep -v '#' | awk -v OFS='\\\\t' '{{print $1,$2,$3,$5}}' | grep -v 'alt' | grep -v 'random' | sort -k 1,1 -k2,2n > {folder}/{genome}.GCpt.bedGraph\"\n",
    "        print(f\"Running {cmd}\")\n",
    "        subprocess.run(cmd, shell=True, check=True)\n",
    "\n",
    "    gcpt_tss_bedgraph_file = f\"{folder}/{genome}.GCpt.tss.bedGraph\"\n",
    "    if not os.path.exists(gcpt_tss_bedgraph_file):\n",
    "        cmd = f\"bedtools map -a {folder}/{genome}.GCpt.bedGraph -b {folder}/{genome}.tss.bed -c 1 -o count -null 0 > {folder}/{genome}.GCpt.tss.bedGraph\"\n",
    "        print(f\"Running {cmd}\")\n",
    "        subprocess.run(cmd, shell=True, check=True)\n",
    "    print(\"GCpt.tss.bedGraph already exists\")\n",
    "    goldenpath = f\"{folder}/{genome}.GCpt.tss.bedGraph\"\n",
    "    return goldenpath\n",
    "\n",
    "def process_and_transpose(gcc_values):\n",
    "    # 将字典中的每个值转换为 DataFrame \n",
    "    dfs = {key: pd.DataFrame(value) for key, value in gcc_values.items()}\n",
    "\n",
    "    # 使用 pd.concat 在轴 1 上合并 DataFrame\n",
    "    result_df = pd.concat(dfs.values(), axis=1)\n",
    "\n",
    "    # 转置 DataFrame\n",
    "    result_df = result_df.transpose()\n",
    "    # 在列名后添加 .cor 后缀\n",
    "    result_df.columns = result_df.columns + '.cor'\n",
    "\n",
    "    # 添加 'name' 列，该列包含原始 DataFrame 的索引\n",
    "    result_df['name'] = result_df.index\n",
    "\n",
    "    return result_df\n",
    "vals_path = 'Python_vals.txt'\n",
    "\n",
    "if os.path.exists(vals_path):\n",
    "    os.remove(vals_path)\n",
    "def run_r_function(i,pc_mat, sam, gcc_values, tss_values, len_values, chr_list):\n",
    "    # 将Python对象转换为R对象\n",
    "    r_pc_mat=pandas2ri.py2rpy(pc_mat)\n",
    "    # r_pc_mat = robjects.r['as.matrix'](r_pc_mat)\n",
    "    sam_length = len(sam)\n",
    "    gcc_values_r = pandas2ri.py2rpy(gcc_values)\n",
    "    tss_values_r = pandas2ri.py2rpy(tss_values)\n",
    "    len_values_r = pandas2ri.py2rpy(len_values)\n",
    "\n",
    "    # 在R环境中定义r_pc_mat\n",
    "    robjects.globalenv['i'] = i\n",
    "    robjects.globalenv['r_pc_mat'] = r_pc_mat\n",
    "    robjects.globalenv['sam_length'] = sam_length\n",
    "    # 在设置 chr_list 到 R 环境之前，将其转换为字符向量\n",
    "    robjects.globalenv['chr_list'] = robjects.StrVector(chr_list)\n",
    "\n",
    "    # robjects.globalenv['chr_list'] = chr_list\n",
    "    robjects.globalenv['gcc_values_r'] = gcc_values_r\n",
    "    robjects.globalenv['tss_values_r'] = tss_values_r\n",
    "    robjects.globalenv['len_values_r'] = len_values_r\n",
    "    \n",
    "\n",
    "    # 执行R代码\n",
    "    robjects.r('''\n",
    "        # print(head(r_pc_mat))\n",
    "        # print(chr_list)\n",
    "        # print(gcc_values_r)\n",
    "    \n",
    "        hcl <- hclust(as.dist(round(1-cor(r_pc_mat),4)))\n",
    "\t\tcl<- list()\n",
    "\t\tk <- 1\n",
    "\t\th <- 0.05   \n",
    "\t\twhile (h < 1) {\n",
    "\t\t\tg <- data.frame(group=cutree(hcl, h=h))\t\n",
    "\t\t\tg[,\"name\"]  <- rownames(g)\n",
    "\t\t\tg[,\"name2\"] <- paste0(g$group,\"-\",do.call(rbind,strsplit(g$name,\"[.]\"))[,1])\n",
    "\t\t\tv <- data.frame(member=table(g$name2))\n",
    "\t\t\tv$member.Var1 <- do.call(rbind,strsplit(as.character(v$member.Var1),\"-\"))[,1]\n",
    "\t\t\tv <- data.frame(member=table(v$member.Var1))\n",
    "            # print(g)\n",
    "\t\t\ts <- v[v$member.Freq == sam_length,]$member.Var1\n",
    "\t\t\tif (length(s) > 0) {\n",
    "\n",
    "\t\t\t\tfor(u in 1:length(s)) {\n",
    "\t\t\t\t\tif (length(unique(g[g$group==s[u],]$name)) == sam_length) {\n",
    "\t\t\t\t\t\tcl[[k]] <- g[g$group==s[u],]\n",
    "\t\t\t\t\t\tk <- k + 1\n",
    "\t\t\t\t\t}\n",
    "\t\t\t\t}\n",
    "\t\t\t}\n",
    "\t\t\th <- h + 0.025\n",
    "            # print(paste(\"Current h value:\", h))\n",
    "            # print(paste(\"Number of groups:\", length(unique(g$group))))\n",
    "            # print(paste(\"Number of unique s values:\", length(unique(s))))\n",
    "\t\t}\n",
    "        if (length(cl) > 0) {\n",
    "\t\t\tcl <- as.data.frame(do.call(rbind, cl))\n",
    "\t\t\tcl <- unique(cl)\n",
    "\t\t\tcl[,\"chr\"] <- chr_list[i+1]\n",
    "\t\t\tcl[,\"gcc.cor\"] <- 0\n",
    "\t\t\tcl[,\"tss.cor\"] <- 0\n",
    "\n",
    "\t\t\tfor(v in 1:nrow(gcc_values_r)) {\n",
    "\t\t\t\tcl[cl$name == gcc_values_r$name[v],\"gcc.cor\"] <- gcc_values_r$gcc.cor[v]\n",
    "\t\t\t\tcl[cl$name == gcc_values_r$name[v],\"tss.cor\"] <- tss_values_r$tss.cor[v]\n",
    "\t\t\t\tcl[cl$name == gcc_values_r$name[v],\"len.cor\"] <- abs(len_values_r$len.cor[v])\n",
    "\t\t\t}\n",
    "\t\t\tdata$clus[[d]] <- cl\n",
    "\t\t\tdata$cor[[d]] <- as.data.frame(aggregate(gcc.cor ~ group + chr, mean, data=cl))\n",
    "\t\t\tdata$cor[[d]][,\"tss.cor\"] <- aggregate(tss.cor ~ group + chr, mean, data=cl)$tss.cor\n",
    "\t\t\tdata$cor[[d]][,\"len.cor\"] <- aggregate(len.cor ~ group + chr, mean, data=cl)$len.cor\n",
    "\t\t\tdata$cor[[d]][,\"score\"] <- apply(data$cor[[d]][,c(\"tss.cor\",\"gcc.cor\")],1,sum)\n",
    "\t\t\tdata$cor[[d]]$gcc.cor <- round(data$cor[[d]]$gcc.cor, 4)\n",
    "\t\t\tdata$cor[[d]]$tss.cor <- round(data$cor[[d]]$tss.cor, 4)\n",
    "\t\t\tdata$cor[[d]]$len.cor <- round(data$cor[[d]]$len.cor, 4)\n",
    "\t\t\tdata$cor[[d]]$score   <- round(data$cor[[d]]$score, 4)\n",
    "\t\t\tdata$chrom[[d]] <- chr_list[i+1]\n",
    "\t\t\td <- d + 1\n",
    "\t\t}\n",
    "        length_cl<-length(cl)''')\n",
    "\n",
    "    # 从R中检索结果\n",
    "    data= robjects.globalenv['data']\n",
    "    length_cl = robjects.globalenv['length_cl']\n",
    "    # data_clus = robjects.globalenv['data'].rx2('clus')\n",
    "    # data_cor = robjects.globalenv['data'].rx2('cor')\n",
    "    # data_chrom = robjects.globalenv['data'].rx2('chrom')\n",
    "    # print(data_clus)\n",
    "    # 将R数据框转换为Pandas数据框\n",
    "    # data_clus = pandas2ri.ri2py(data_clus)\n",
    "    # data_cor = pandas2ri.ri2py(data_cor)\n",
    "    # data_chrom = pandas2ri.ri2py(data_chrom)\n",
    "    # 使用提取数据的方法\n",
    "    # data_clus_df = pandas2ri.rpy2py_dataframe(data_clus)\n",
    "    # data_cor_df = pandas2ri.rpy2py_dataframe(data_cor)\n",
    "    # data_chrom_df = pandas2ri.rpy2py_dataframe(data_chrom)\n",
    "\n",
    "    return data,length_cl\n",
    "robjects.r('''\n",
    "        d <- 1\n",
    "        data <- list(clus=list(),cor=list(),chrom=list())\n",
    "        ''')\n",
    "\n",
    "\n",
    "genome_name = \"mm10\"\n",
    "resolution_value = 250000\n",
    "pc_k = 2\n",
    "data_folder = None\n",
    "#下载参考基因组文件并生成 GCpt.tss.bedGraph文件（用于后续挑选pc）\n",
    "goldenpath = download_and_extract(genome_name, data_folder, resolution_value)\n",
    "group_id = 'compartment_raw'\n",
    "df = '/home/python/higashi/dataset_hic/dataset2/cortex250k/compartment_raw.h5'\n",
    "file = h5py.File(df, 'r')\n",
    "bin_chr = np.array([chrom.decode('utf-8') for chrom in file['compartment_raw']['bin']['chrom']])\n",
    "bin_start = file[group_id + '/bin/start'][:]\n",
    "bin_end = file[group_id + '/bin/end'][:]\n",
    "\n",
    "chr_list = list(np.unique(bin_chr))\n",
    "chr_list.sort(key=lambda l: int(re.findall('\\d+', l)[0]))\n",
    "# chr_list = ['chr1','chr2']\n",
    "print(chr_list)\n",
    "vals = {}\n",
    "\n",
    "cell_id = [1,2,3]\n",
    "sam = ['cell_' + str(cell) for cell in cell_id]\n",
    "\n",
    "for i in range(len(chr_list)):\n",
    "    chr_index = np.where(bin_chr == chr_list[i])[0]\n",
    "    gcc_values = {}\n",
    "    tss_values = {}\n",
    "    len_values = {}\n",
    "    chrom_list = {}\n",
    "    count_vect = {}\n",
    "\n",
    "    for j in range(len(sam)):\n",
    "        print(f\"在 {sam[j]} 样本中运行 {chr_list[i]}\")\n",
    "        cell_data = file[group_id + f'/cell_{j}/'][:]\n",
    "        #按染色体名取对应位置数据\n",
    "        pca = pd.DataFrame({'chr': bin_chr[chr_index], 'start': bin_start[chr_index], 'end': bin_end[chr_index],'index': bin_end[chr_index]/resolution_value})\n",
    "        lhs = pd.DataFrame(data=cell_data)\n",
    "        # print(pca.shape)\n",
    "        pca[['pc1', 'pc2', 'pc3']] = lhs.iloc[chr_index, :].values\n",
    "        pca =pca.iloc[:,0:(4+pc_k)]\n",
    "        \n",
    "        #添加行名如：chr_250000\n",
    "        pca.index = pca[\"chr\"].astype(str) + \"_\" + pca[\"start\"].astype(str)\n",
    "        \n",
    "        gcc = pd.read_table(goldenpath, header=None, names=[\"chr\", \"start\", \"end\", \"gcc\", \"tss\"])\n",
    "        # 为GCC数据添加行名\n",
    "        gcc.index = gcc[\"chr\"].astype(str) + \"_\" + gcc[\"start\"].astype(str)\n",
    "\n",
    "        # print(pca)\n",
    "\n",
    "        # 将GCC和TSS信息添加到PCA中\n",
    "        pca[[\"gcc\", \"tss\"]] = gcc.loc[pca.index, [\"gcc\", \"tss\"]]\n",
    "        pca['len'] = range(1, pca.shape[0] + 1)\n",
    "        # print(pca.iloc[:, 4])\n",
    "        # 生成bedGraph文件\n",
    "        for k in range(4, pca.shape[1] - 3):\n",
    "\n",
    "            # 计算pc值和gcc的相关性 决定pc值是否乘以-1\n",
    "            pca.iloc[:, k] = np.sign(pearsonr(pca.iloc[:, k], pca[\"gcc\"])[0])*pca.iloc[:, k]\n",
    "\n",
    "        # 存储染色体的pc值(正负校正后)\n",
    "        chrom_list[j] = pca.iloc[:, 4:(pca.shape[1] - 3)]\n",
    "        \n",
    "        colnames_chrom = [f\"{sam[j]}.PC{i}\" for i in range(1, chrom_list[j].shape[1] + 1)]\n",
    "        # print(colnames_chrom)\n",
    "        chrom_list[j].columns = colnames_chrom\n",
    "\n",
    "        # 存储行名\n",
    "        count_vect[j] = chrom_list[j].index\n",
    "        # print(count_vect[j])\n",
    "        # 计算pc值和GCC、TSS、行数的相关性\n",
    "        gcc_values[j]=[pd.concat([chrom_list[j], pca[\"gcc\"]], axis=1).corr().iloc[:-1, -1].round(4).transpose()]\n",
    "        # print(gcc_values)\n",
    "        tss_values[j]=[pd.concat([chrom_list[j], pca[\"tss\"]], axis=1).corr().iloc[:-1, -1].round(4).transpose()]\n",
    "        len_values[j]=[pd.concat([chrom_list[j], pca[\"len\"]], axis=1).corr().iloc[:-1, -1].round(4).transpose()]\n",
    "        # print(gcc_values[j])\n",
    "\n",
    "    gcc_df=process_and_transpose(gcc_values)\n",
    "    tss_df = process_and_transpose(tss_values)\n",
    "    len_df = process_and_transpose(len_values)\n",
    "    #合并相关性结果\n",
    "    vals[i] = pd.DataFrame()\n",
    "    vals[i]['gcc.cor'] = gcc_df['gcc.cor']\n",
    "    vals[i]['tss.cor'] = tss_df['tss.cor']\n",
    "    vals[i]['len.cor'] = len_df['len.cor']\n",
    "    vals[i]['chr'] = [chr_list[i]]*len(gcc_df['gcc.cor'])\n",
    "    vals[i]['sample'] = vals[i].index.str.split('.').str[0]\n",
    "    vals[i]['pc']= vals[i].index.str.split('.').str[1]\n",
    "\n",
    "    vals[i].to_csv(vals_path, sep=\"\\t\", header = False,index=False,mode='a')\n",
    "    # print(vals[i])\n",
    "\n",
    "    # print(count_vect[0]+count_vect[1])\n",
    "    #字典中的列表转换成pd.Series并合并成一列\n",
    "    # values, counts = np.unique( pd.concat([pd.Series(count_vect[0]), pd.Series(count_vect[1])], ignore_index=True), return_counts=True)\n",
    "    #改成适用于一个或多个列表\n",
    "    values, counts = np.unique(pd.concat([pd.Series(lst) for lst in count_vect.values()], ignore_index=True), return_counts=True)\n",
    "    # 将结果转换为数据框\n",
    "    count_vect_df = pd.DataFrame({'value': values, 'freq': counts})\n",
    "    # print(count_vect_df)\n",
    "    # print(count_vect_df)\n",
    "    pc_mat=[]\n",
    "    # 遍历 sam 列表的每个元素\n",
    "  \n",
    "    for j in range(len(sam)):\n",
    "        # 选择 chrom_list 中的第 j 个元素的子集\n",
    "        subset = chrom_list[j].loc[count_vect_df['value'], :]\n",
    "\n",
    "        # 将子集添加到 pc_mat 列表中\n",
    "        pc_mat.append(subset)\n",
    "    # 使用 numpy.c_ 将 pc_mat 列表中的矩阵按列合并\n",
    "    pc_mat_combined = np.c_[tuple(pc_mat)]\n",
    "\n",
    "    # 将结果转换为 DataFrame 行索引使用count_vect_df['value']列 列名使用vals[i].index\n",
    "    pc_mat_df = pd.DataFrame(pc_mat_combined,index=count_vect_df['value'],columns=vals[i].index)\n",
    "\n",
    "\n",
    "    data,length_cl = run_r_function(i,pc_mat_df, sam, gcc_df, tss_df, len_df, chr_list)\n",
    "\n",
    "robjects.r('''\n",
    "    clus.df <- as.data.frame(unique(do.call(rbind,data$clus)))\n",
    "\tcor.df  <- as.data.frame(do.call(rbind,data$cor))\n",
    "\tchr.vec <- unlist(data$chrom)\n",
    "        ''')\n",
    "clus_df = pandas2ri.rpy2py_dataframe(robjects.globalenv['clus.df'])\n",
    "cor_df = pandas2ri.rpy2py_dataframe(robjects.globalenv['cor.df'])\n",
    "\n",
    "#############################处理完所有染色体后\n",
    "chr_max = []\n",
    "for i in range(len(chr_list)):\n",
    "    cor_df_chrom = cor_df[cor_df['chr']==chr_list[i]]\n",
    "    clus_df_chrom = clus_df[clus_df['chr']==chr_list[i]]\n",
    "    if len(cor_df_chrom)>0:\n",
    "        selected_rows = cor_df_chrom.loc[cor_df_chrom['score'].idxmax()]\n",
    "        name = clus_df_chrom[clus_df_chrom['group']==selected_rows['group']]['name'].str.split('[.]', expand=True)\n",
    "\n",
    "    #    name_split = selected_rows['name'].str.split('[.]', expand=True)\n",
    "\n",
    "\n",
    "        chr_max.append(pd.DataFrame({\n",
    "            'group': selected_rows['group'],\n",
    "            'chr': selected_rows['chr'],\n",
    "            'gcc.cor': selected_rows['gcc.cor'],\n",
    "            'tss.cor': selected_rows['tss.cor'],\n",
    "            'len.cor': selected_rows['len.cor'],\n",
    "            'sample': [','.join(name[0])],\n",
    "            'pcs': [','.join(map(str, name[1]))]\n",
    "        }))\n",
    "    # print(chr_list[i])\n",
    "    else:\n",
    "        vals_chrom = vals[i]\n",
    "        vals_chrom['score'] = vals_chrom['gcc.cor']+vals_chrom['tss.cor']\n",
    "        # print(vals_chrom)\n",
    "        sample_df = {}\n",
    "        #选择每个sample中score最大的pc所在的行\n",
    "        for j in range(len(sam)):\n",
    "            sample_df[j] = vals_chrom[vals_chrom['sample']==sam[j]]\n",
    "            \n",
    "            sample_df[j] = sample_df[j].loc[sample_df[j]['score'].idxmax()]\n",
    "            \n",
    "            # print(type(sample_df[j]))\n",
    "        #按行合并并转换成数据帧\n",
    "        sample_df = pd.DataFrame(pd.concat(sample_df,axis=1).T)\n",
    "        # print(sample_df)\n",
    "        # 计算每列的均值\n",
    "        gcc_cor_mean = round(sample_df['gcc.cor'].mean(), 4)\n",
    "        tss_cor_mean = round(sample_df['tss.cor'].mean(), 4)\n",
    "        len_cor_mean = round(sample_df['len.cor'].mean(), 4)\n",
    "        \n",
    "        # 将结果添加到 chr_max 列表中\n",
    "        chr_max.append(pd.DataFrame({\n",
    "            'group': [1],\n",
    "            'chr': [chr_list[i]],\n",
    "            'gcc.cor': [gcc_cor_mean],\n",
    "            'tss.cor': [tss_cor_mean],\n",
    "            'len.cor': [len_cor_mean],\n",
    "            'sample': [','.join(sample_df['sample'])],\n",
    "            'pcs': [','.join(map(str, sample_df['pc']))]\n",
    "        }))\n",
    "# 使用 pd.concat 将列表中的数据框按行合并\n",
    "chr_max_df = pd.concat(chr_max, ignore_index=True)\n",
    "\n",
    "# 将数据框写入文本文件\n",
    "chr_max_df.to_csv(\"Python_test_chr_pc_selected1.txt\", sep=\"\\t\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
